<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>机器学习基石上（第2周）学习笔记--Learning to Answer Yes/No</title>
      <link href="/2018/04/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E4%B8%8A%EF%BC%88%E7%AC%AC2%E5%91%A8%EF%BC%89%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/04/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E4%B8%8A%EF%BC%88%E7%AC%AC2%E5%91%A8%EF%BC%89%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h1 id="上一周回顾"><a href="#上一周回顾" class="headerlink" title="上一周回顾"></a>上一周回顾</h1><p>第一周题目为 The Learning Problem，主要讲了机器学习的定义和应用，并介绍了机器学习流程，如下图所示。<br>训练样本D，使用算法A，在假设模型集中选择最优的，训练后得到 g（假设函数）≈f(目标函数)。<br><img src="https://img-blog.csdn.net/20170607153730795?" alt="avatar"><br>本周主要讲解PLA(感知机算法)，即二分类算法之一。</p><h1 id="1-Perceptron-Hypothesis-Set"><a href="#1-Perceptron-Hypothesis-Set" class="headerlink" title="1.Perceptron Hypothesis Set"></a>1.Perceptron Hypothesis Set</h1><p>继续以上一周的信用卡申请为例，如下图。银行根据用户的年龄、年收入、工作年限、负债等情况来判断是否给该用户发信用卡。<br><img src="https://img-blog.csdn.net/20160512221250323?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="avatar"><br>现在的训练集D：历史用户的信息和是否发了信用卡。在机器学习流程中，在模型集 H 中选择合适的模型非常重要。下面要讲解的Hypothesis Set:感知机（Perceptron）。</p><p>上面的例子，</p><pre><code>个人信息 = 特征向量x(i)有d个信息 = d个特征向量（特征集为d维）每个特征向量给定权重w(i) =&gt; 该特征向量对结果（是否发信用卡）的影响加权特征向量的和 &gt; 设定的阈值threshold =&gt; y=1,通过加权特征向量的和 &lt; 设定的阈值threshold =&gt; y=-1,拒绝感知机模型 h(x) = （加权特征向量 - 阈值threshold）的正负性</code></pre><p>从而我们的目标：权值w（d维）和 阈值threshold。</p><p>下面对 h(x)转换，令阈值 threshold=w0，引入一个 x0=1 的特征向量与 w0 相乘，这样阈值也成了特征为1的特征向量（即成了个人信息之一），此时为 （d+1）个特征，如下图。<br> <img src="https://img-blog.csdn.net/20170608083119699?" alt="avatar"><br>假设感知机模型在二维平面上，即h(x)=sign(w0+w1x1+w2x2)。其中，w0+w1x1+w2x2=0 是平面上一条分类直线，直线一侧是正类（+1），直线另一侧是负类（-1）。权重w不同，对应于平面上不同的直线。如下图。<br><img src="https://img-blog.csdn.net/20170608084125366?" alt="avatar"><br>在上图中，感知机模型就是一条蓝色的直线，称之为linear(binary) classifiers。在3维中，感知机模型就是平面，在更高维度中，感知机模型（线性分类）用超平面表示，即只要是形如 wTx 的模型就都属于linear(binary) classifiers。</p><p>本例的linear(binary) classifiers是用简单的感知器模型建立的，线性分类问题还可以使用logistic regression(逻辑回归)。</p><h1 id="2-Perceptron-Learning-Algorithm-PLA"><a href="#2-Perceptron-Learning-Algorithm-PLA" class="headerlink" title="2.Perceptron Learning Algorithm(PLA)"></a>2.Perceptron Learning Algorithm(PLA)</h1><p>通过上一节的介绍，大致了解了感知机模型，在上例中，hypothesis set就是一系列直线。得到的hypothesis set，下一步的目标就是通过算法A找到最优的直线。</p><p>为了找到这条最优直线，我们可以使用“逐步修正”，具体如下：</p><pre><code>1.在平面上任意取一条直线 h(x)，法向量为w12.遍历平面上各点带入 h(x),碰到第一个符号相反的点就停下3.实际y=+1,而h(x)判断为负，说明直线的 w1 与 x（向量形式）夹角大于90度，进行法向量修正，令 w2 = w1 + x(1),修正后直线的w2 与 x 夹角 小于90度，为正，修正完成。实际y=-1,而h(x)判断为正同理。4.接着重复2,3步骤，不断迭代，直到所有的点都完全分类正确。</code></pre><p>下图是步骤3的具体操作。<br><img src="https://img-blog.csdn.net/20170608095000165?" alt="avatar"><br>上面“逐步修正”就是本周重点学习的PLA算法。<br>实际操作中，可以一个点一个点地遍历，发现分类错误的点就进行修正，直到所有点全部分类正确。这种被称为Cyclic PLA。<br><img src="https://img-blog.csdn.net/20170608102847562?" alt="avatar"><br>下面用图解的形式来介绍PLA的修正过程：红色是法向量w,黑色的犯错的点x,紫色为修正的w。<br><img src="https://img-blog.csdn.net/20170608104910590?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608104952086?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608105013685?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608105029404?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608105044842?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608105044842?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608105100764?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608105122249?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608105214946?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608105230634?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608105243181?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608105257829?" alt="avatar"><br>对PLA，我们需要考虑以下两个问题：</p><ul><li>PLA迭代一定会停下来吗？如果线性不可分怎么办？</li><li>PLA停下来的时候，是否能保证f≈g？如果没有停下来，是否有f≈g？</li></ul><p>下面一节对上诉问题给出解决方案。</p><h1 id="3-Guarantee-of-PLA"><a href="#3-Guarantee-of-PLA" class="headerlink" title="3.Guarantee of PLA"></a>3.Guarantee of PLA</h1><p>根据PLA的定义，当找到一条直线，能将所有平面上的点都分类正确，那么PLA就停止了。要达到这个终止条件，就必须保证D是线性可分（linear separable）。如果是非线性可分的，那么，PLA就不会停止。如下图。<br><img src="https://img-blog.csdn.net/20170608111542131?" alt="avatar"><br>如果D线性可分，那么存在一条直线可以将正类和负类分开，令这时候的目标权重为wf，即对任一点都有：<br><img src="https://img-blog.csdn.net/20170608134312092?" alt="avatar"><br>PLA会对每次错误的点进行修正，更新权重wt+1的值，如果wt+1与wf越来越接近，数学运算上就是内积越大，那表示wt+1是在接近目标权重wf，证明PLA是有学习效果的。所以，我们来计算wt+1与wf的内积：<br><img src="https://img-blog.csdn.net/20170608134340499?" alt="avatar"><br>从推导可以看出，wt+1与wf的内积跟wt与wf的内积相比更大了。似乎说明了wt+1更接近wf，但是内积更大，可能是向量长度更大了，不一定是向量间角度更小。所以，下一步，我们还需要证明wt+1与wt向量长度的关系：<br><img src="https://img-blog.csdn.net/20160704161452151?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608140302480?" alt="avatar"><br>wt只会在分类错误的情况下更新，最终得到的||w2t+1||相比||w2t||的增量值不超过max||x2n||。也就是说，wt的增长被限制了，wt+1与wt向量长度不会差别太大！<br>如果令初始权值w0=0，那么经过T次错误修正后，有如下结论：<br><img src="https://img-blog.csdn.net/20160704154757801?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="avatar"><br>上图中公式1给出了WT的模的求法，并且最终迭代得：<br><img src="https://img-blog.csdn.net/20160704162324944?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="avatar"><br>进一步：<br><img src="https://img-blog.csdn.net/20160704163407931?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="avatar"><br>可以看出权值向量和目标函数内积会以的速度不断的增长，但是这种增长不是没有限制的，它最多只能等于1。<br>进而：<br><img src="https://img-blog.csdn.net/20160704163907886?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="avatar"><br>上面看不懂得，下面贴出了更详细的推导：<br><img src="https://img-blog.csdn.net/20170608143421951?" alt="avatar"><br><img src="https://img-blog.csdn.net/20170608143438779?" alt="avatar"> </p><h1 id="4-Non-Separable-Data"><a href="#4-Non-Separable-Data" class="headerlink" title="4.Non-Separable Data"></a>4.Non-Separable Data</h1><p>上节我们证明了线性可分的情况下，迭代是有限的。本节解决线性不可分的情况。如果一个PLA算法运行了很长时间仍然没有停止，此时存在两种可能性，一是该数据集是线性可分的，但是还没有运行结束；另一种，压根就不存在一条直线可以将数据集分开，就是压根这个算法就不会终止。假如是后者又该如何处理？</p><p>出现不可分的一种可能是从未知目标函数中产生的训练样本存在噪音（noise），如录入样本时有人工的错误等情况导致数据本身不正确，使得最终本可以线性可分的样本集变得线性不可分了。这时，机器学习流程是这样的：<br><img src="https://img-blog.csdn.net/20170608150716294?" alt="avatar"><br>在非线性情况下，我们可以把条件放松，即不苛求每个点都分类正确，而是容忍有错误点，取错误点的个数最少时的权重w：<br><img src="https://img-blog.csdn.net/20170608151418751?" alt="avatar"><br>事实证明，上面的解是NP-hard问题，难以求解。然而，我们可以对在线性可分类型中表现很好的PLA做个修改，把它应用到非线性可分类型中，获得近似最好的g。</p><p>修改后的PLA称为Packet Algorithm。它的算法流程与PLA基本类似，首先初始化权重w0，计算出在这条初始化的直线中，分类错误点的个数。然后对错误点进行修正，更新w，得到一条新的直线，在计算其对应的分类错误的点的个数，并与之前错误点个数比较，取个数较小的直线作为我们当前选择的分类直线。之后，再经过n次迭代，不断比较当前分类错误点个数与之前最少的错误点个数比较，选择最小的值保存。直到迭代次数完成后，选取个数最少的直线对应的w，即为我们最终想要得到的权重值。<br><img src="https://img-blog.csdn.net/20170608155259223?" alt="avatar"><br>如何判断数据集D是不是线性可分？对于二维数据来说，通常还是通过肉眼观察来判断的。一般情况下，Pocket Algorithm要比PLA速度慢一些。</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>机器学习基石上（第1周）学习笔记--The learning Problem</title>
      <link href="/2018/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E4%B8%8A%EF%BC%88%E7%AC%AC1%E5%91%A8%EF%BC%89%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E4%B8%8A%EF%BC%88%E7%AC%AC1%E5%91%A8%EF%BC%89%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<h1 id="1-课程介绍"><a href="#1-课程介绍" class="headerlink" title="1.课程介绍"></a>1.课程介绍</h1><p>林轩田老师介绍了“机器学习基石（上）”的课程安排。</p><h1 id="2-什么是机器学习"><a href="#2-什么是机器学习" class="headerlink" title="2.什么是机器学习"></a>2.什么是机器学习</h1><p>学习：获取技能</p><pre><code>观察 -&gt;学习 -&gt;技能</code></pre><p> 机器学习：获取技能</p><pre><code>数据 -&gt;机器学习(ML) -&gt;技能</code></pre><p> 而</p><pre><code>技能 &lt;=&gt; 提高某种表现能力</code></pre><p> 机器学习也可以是</p><pre><code>数据 -&gt;机器学习(ML) -&gt;提高某种表现能力</code></pre><p> 例如机器学习在金融领域的应用：</p><pre><code>股票数据 -&gt;机器学习(ML) -&gt;提高收益</code></pre><h1 id="3-机器学习应用"><a href="#3-机器学习应用" class="headerlink" title="3.机器学习应用"></a>3.机器学习应用</h1><p> 日常需要：吃、穿、住、行<br> 比如电影推介系统：<br> 数据：用户给电影评分数据<br> 能力：预测一个用户给某部未评分的电影打分</p><p><img src="\\images\pasted-1.png\" alt="upload successful"></p><pre><code>用户对电影的喜好特征 &lt;----进行匹配（内积等）----&gt; 电影的自带特征</code></pre><p>这样就能预测用户对某部电影的评分，判断是否给该用户推荐此电影</p><h1 id="4-机器学习组成部分"><a href="#4-机器学习组成部分" class="headerlink" title="4.机器学习组成部分"></a>4.机器学习组成部分</h1><p>例如，申请银行信用卡，下图是用户填的信息：</p><p><img src="\\images\pasted-2.png\" alt="upload successful"></p><p>而银行可以根据这些信息判断是否给该用户发信用卡。<br>上面的例子就有机器学习的基本组成：</p><pre><code>输入：x ∈ X （用户信息）输出：y ∈ Y （银行是否发卡）目标函数：f: X -&gt; Y （理想信用卡审核模型）数据 &lt;=&gt; 训练集： D = {(x1, y1), (x2, y2), ... ,(xn, yn)} （银行信用卡历史申请人数据）假设（hypothesis）&lt;=&gt; 能力：g:X -&gt; Y（通过数据学习的信用卡审核模型）</code></pre><p>通常情况下 g 与 f 越接近越好。</p><pre><code>未知目标函数：f: X -&gt; Y|训练集D -&gt; 学习算法A -&gt; hypothesis 函数 g ≈ f</code></pre><p>一般机器学习模型如下图所示：<br><img src="\\images\pasted-3.png\" alt="upload successful"></p><h1 id="5-数据挖掘"><a href="#5-数据挖掘" class="headerlink" title="5.数据挖掘"></a>5.数据挖掘</h1><p>机器学习（使用数据构建接近于f的hypothesis函数g）<br>≈ 数据挖掘（利用大量数据找到感兴趣的东西）</p><p>机器学习是人工智能（AI）的实现方法之一。</p><h1 id="课后练习答案"><a href="#课后练习答案" class="headerlink" title="课后练习答案"></a>课后练习答案</h1><h2 id="1-概率与统计"><a href="#1-概率与统计" class="headerlink" title="1. 概率与统计"></a>1. 概率与统计</h2><h3 id="1-1-证明组合数公式"><a href="#1-1-证明组合数公式" class="headerlink" title="1.1. 证明组合数公式"></a>1.1. 证明组合数公式</h3><pre><code>C(n,m)=n!/((n-m)!*m!)</code></pre><p>条件2实际上是组合数的性质2：</p><pre><code>C(n,m)=C(n-1,m-1)+C(n-1,m)</code></pre><p>通过性质2反证即可。</p><h3 id="1-2-组合数的应用于概率"><a href="#1-2-组合数的应用于概率" class="headerlink" title="1.2. 组合数的应用于概率"></a>1.2. 组合数的应用于概率</h3><h3 id="1-3-条件概率"><a href="#1-3-条件概率" class="headerlink" title="1.3. 条件概率"></a>1.3. 条件概率</h3><h3 id="1-4-贝叶斯定理"><a href="#1-4-贝叶斯定理" class="headerlink" title="1.4. 贝叶斯定理"></a>1.4. 贝叶斯定理</h3><h3 id="1-5-交集和并集"><a href="#1-5-交集和并集" class="headerlink" title="1.5. 交集和并集"></a>1.5. 交集和并集</h3><h2 id="2-线性代数"><a href="#2-线性代数" class="headerlink" title="2. 线性代数"></a>2. 线性代数</h2><h3 id="2-1-矩阵的秩"><a href="#2-1-矩阵的秩" class="headerlink" title="2.1. 矩阵的秩"></a>2.1. 矩阵的秩</h3><h3 id="2-2-逆矩阵"><a href="#2-2-逆矩阵" class="headerlink" title="2.2. 逆矩阵"></a>2.2. 逆矩阵</h3><h3 id="2-3-特征值和特征向量"><a href="#2-3-特征值和特征向量" class="headerlink" title="2.3. 特征值和特征向量"></a>2.3. 特征值和特征向量</h3><h3 id="2-4-矩阵的秩"><a href="#2-4-矩阵的秩" class="headerlink" title="2.4. 矩阵的秩"></a>2.4. 矩阵的秩</h3><h3 id="2-5-正定矩阵和非正定矩阵"><a href="#2-5-正定矩阵和非正定矩阵" class="headerlink" title="2.5. 正定矩阵和非正定矩阵"></a>2.5. 正定矩阵和非正定矩阵</h3><h3 id="2-6-矩阵的乘"><a href="#2-6-矩阵的乘" class="headerlink" title="2.6. 矩阵的乘"></a>2.6. 矩阵的乘</h3><h2 id="3-微积分"><a href="#3-微积分" class="headerlink" title="3. 微积分"></a>3. 微积分</h2><h3 id="3-1-导数和偏导数"><a href="#3-1-导数和偏导数" class="headerlink" title="3.1 导数和偏导数"></a>3.1 导数和偏导数</h3><h3 id="3-2-复合导数求偏导"><a href="#3-2-复合导数求偏导" class="headerlink" title="3.2 复合导数求偏导"></a>3.2 复合导数求偏导</h3><h3 id="3-3-梯度和hession"><a href="#3-3-梯度和hession" class="headerlink" title="3.3 梯度和hession"></a>3.3 梯度和hession</h3><h3 id="3-4-泰勒展开式"><a href="#3-4-泰勒展开式" class="headerlink" title="3.4 泰勒展开式"></a>3.4 泰勒展开式</h3><h3 id="3-5-优化（极值）"><a href="#3-5-优化（极值）" class="headerlink" title="3.5 优化（极值）"></a>3.5 优化（极值）</h3><h3 id="3-6-矢量微积分"><a href="#3-6-矢量微积分" class="headerlink" title="3.6 矢量微积分"></a>3.6 矢量微积分</h3><p>上面的作业就是学习本课程的基础数学知识点。</p>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Markdowm简明语法学习</title>
      <link href="/2018/03/26/Hello-github/"/>
      <url>/2018/03/26/Hello-github/</url>
      <content type="html"><![CDATA[<h3 id="1-斜体和粗体"><a href="#1-斜体和粗体" class="headerlink" title="1.斜体和粗体"></a>1.斜体和粗体</h3><p>使用 * 和 ** 分别表示斜体和粗体</p><p><em>斜体</em> <strong>粗体</strong></p><h3 id="2-分级标题"><a href="#2-分级标题" class="headerlink" title="2. 分级标题"></a>2. 分级标题</h3><p>使用 === 表示一级标题，使用 — 表示二级标题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这是一个一级标题</span><br><span class="line">==========</span><br><span class="line">这是一个二级标题</span><br><span class="line">----------</span><br><span class="line">### 这是一个三级标题</span><br></pre></td></tr></table></figure><p>你也可以选择在行首加井号表示不同级别的标题 (H1-H6)，<br>例如：# H1, ## H2, ### H3，#### H4。</p><h3 id="3-跳转url"><a href="#3-跳转url" class="headerlink" title="3. 跳转url"></a>3. 跳转url</h3><p>使用 [描述](链接地址) 为文字增加外链接。<br><a href="https://github.com/Junezm/junezm.github.io.git" target="_blank" rel="noopener">我的blog地址</a></p><h3 id="4-列表"><a href="#4-列表" class="headerlink" title="4. 列表"></a>4. 列表</h3><h4 id="4-1-有序列表"><a href="#4-1-有序列表" class="headerlink" title="4.1 有序列表"></a>4.1 有序列表</h4><p>使用数字、点加空格表示有序列表。</p><ol><li>有序列表项 一</li><li>有序列表项 二</li><li>有序列表项 三<h4 id="4-2-无序列表"><a href="#4-2-无序列表" class="headerlink" title="4.2 无序列表"></a>4.2 无序列表</h4>使用 *，+，- 表示无序列表。</li></ol><ul><li>无序列表1</li><li>无序列表3</li><li>无序列表2</li></ul><ul><li>啊哈哈哈</li></ul><ul><li>6666666</li></ul><h3 id="5-文字引用"><a href="#5-文字引用" class="headerlink" title="5. 文字引用"></a>5. 文字引用</h3><p>使用 &gt; 表示文字引用。</p><blockquote><p>塞纳河畔， 左岸的咖啡</p></blockquote><h3 id="6-代码"><a href="#6-代码" class="headerlink" title="6. 代码"></a>6. 代码</h3><h4 id="6-1-行内代码"><a href="#6-1-行内代码" class="headerlink" title="6.1 行内代码"></a>6.1 行内代码</h4><p>学习<code>markdown</code>语法</p><h4 id="6-2-块代码"><a href="#6-2-块代码" class="headerlink" title="6.2 块代码"></a>6.2 块代码</h4><p>使用 四个缩进空格 即<code>tab</code>键表示代码块。</p><pre><code>let a = 5;let b = 3;let foo = (a, b) =&gt; a+b</code></pre><h3 id="7-插入图片"><a href="#7-插入图片" class="headerlink" title="7. 插入图片"></a>7. 插入图片</h3><p><img src="https://t11.baidu.com/it/u=261204805,3272339267&amp;fm=173&amp;app=25&amp;f=JPEG?w=639&amp;h=426&amp;s=98B3F1B474099EEF6C105E090300E0DC" alt="我的图片"></p>]]></content>
      
      
        <tags>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
